{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307adbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fetch] page 1: https://github.com/google?tab=repositories\n",
      "[Parsed] new repos: 10, total: 10\n",
      "[Stop] No next page.\n",
      "Scraped 10 repositories.\n",
      "Saved repos:\n",
      "('google/or-tools', 'C++', 12715)\n",
      "('google/perfetto', 'C++', 5019)\n",
      "('google/angle', 'C++', 3843)\n",
      "('google/XNNPACK', 'C', 2181)\n",
      "('google/tunix', 'Python', 1916)\n",
      "('google/nomulus', 'Java', 1768)\n",
      "('google/yggdrasil-decision-forests', 'C++', 622)\n",
      "('google/osv-scalibr', 'Go', 538)\n",
      "('google/orbax', 'Python', 455)\n",
      "('google/skia-buildbot', 'Go', 158)\n",
      "('google/device-infra', 'Java', 58)\n",
      "('google/dwh-migration-tools', 'Java', 54)\n",
      "('google/koladata', 'C++', 27)\n",
      "('google/kotlin-fhirpath', 'Kotlin', 6)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "import time\n",
    "import sqlite3\n",
    "\n",
    "BASE_URL = \"https://github.com\"\n",
    "ORG_REPOS_URL = \"https://github.com/google?tab=repositories\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "DB_PATH = \"google_repos.db\"\n",
    "\n",
    "def init_db():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS repos (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT NOT NULL UNIQUE,\n",
    "            language TEXT,\n",
    "            stars INTEGER NOT NULL\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def parse_star_count(text):\n",
    "    s = text.strip().lower().replace(\",\", \"\")\n",
    "    if not s:\n",
    "        return 0\n",
    "    try:\n",
    "        if s.endswith(\"k\"):\n",
    "            return int(float(s[:-1]) * 1000)\n",
    "        if s.endswith(\"m\"):\n",
    "            return int(float(s[:-1]) * 1000000)\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def normalize_repo_full_name(href_text, link_text):\n",
    "    href = (href_text or \"\").strip()\n",
    "    txt = (link_text or \"\").strip()\n",
    "    full = None\n",
    "    if href:\n",
    "        parsed = urlparse(href)\n",
    "        path = parsed.path if parsed.scheme else href\n",
    "        path = path.strip(\"/\")\n",
    "        if path:\n",
    "            full = path\n",
    "    if not full:\n",
    "        if \"/\" in txt:\n",
    "            full = txt.strip()\n",
    "        elif txt:\n",
    "            full = f\"google/{txt}\"\n",
    "    return full\n",
    "\n",
    "def extract_repos_from_page(soup):\n",
    "    repos = []\n",
    "\n",
    "    candidates = []\n",
    "    candidates.extend(soup.select(\"li[itemprop='owns']\"))                    # 旧UI\n",
    "    if not candidates:\n",
    "        candidates.extend(soup.select(\"div[data-testid='results-list'] li\")) # 新UI候補\n",
    "    if not candidates:\n",
    "        candidates.extend(soup.select(\"li.Box-row\"))                         # 汎用\n",
    "    if not candidates:\n",
    "        for li in soup.select(\"li\"):\n",
    "            if li.select_one(\"a[href*='/google/'], a[data-hovercard-type='repository']\"):\n",
    "                candidates.append(li)\n",
    "\n",
    "    for li in candidates:\n",
    "        name_a = li.select_one(\n",
    "            \"h3 a, a[itemprop='name codeRepository'], a[data-hovercard-type='repository'], a[href*='/google/']\"\n",
    "        )\n",
    "        if not name_a:\n",
    "            continue\n",
    "\n",
    "        full_name = normalize_repo_full_name(name_a.get(\"href\"), name_a.get_text())\n",
    "        if not full_name or \"/\" not in full_name:\n",
    "            continue\n",
    "\n",
    "        lang_el = (\n",
    "            li.select_one(\"[itemprop='programmingLanguage']\")\n",
    "            or li.select_one(\"span[data-testid='repo-language-color'] + span\")\n",
    "        )\n",
    "        language = lang_el.get_text(strip=True) if lang_el else None\n",
    "        if language == \"\":\n",
    "            language = None\n",
    "\n",
    "        star_el = (\n",
    "            li.select_one(\"a[href$='/stargazers']\")\n",
    "            or li.select_one(\"a.Link--muted[href$='/stargazers']\")\n",
    "        )\n",
    "        stars = 0\n",
    "        if star_el:\n",
    "            stars = parse_star_count(star_el.get_text(strip=True))\n",
    "\n",
    "        repos.append({\"name\": full_name, \"language\": language, \"stars\": stars})\n",
    "\n",
    "    return repos\n",
    "\n",
    "def get_next_page_url(soup, current_url):\n",
    "    next_el = (\n",
    "        soup.select_one(\"a.next_page\")\n",
    "        or soup.select_one(\"a[rel='next']\")\n",
    "        or soup.select_one(\"nav[aria-label='Pagination'] a[aria-label='Next']\")\n",
    "    )\n",
    "    if next_el and next_el.get(\"href\"):\n",
    "        return urljoin(BASE_URL, next_el[\"href\"])\n",
    "\n",
    "    parsed = urlparse(current_url)\n",
    "    qs = parsed.query.split(\"&\") if parsed.query else []\n",
    "    page_val = None\n",
    "    new_qs = []\n",
    "    for q in qs:\n",
    "        if q.startswith(\"page=\"):\n",
    "            try:\n",
    "                page_val = int(q.split(\"=\", 1)[1])\n",
    "                new_qs.append(f\"page={page_val+1}\")\n",
    "            except ValueError:\n",
    "                page_val = None\n",
    "        else:\n",
    "            new_qs.append(q)\n",
    "\n",
    "    if page_val is not None:\n",
    "        new_query = \"&\".join([p for p in new_qs if p])\n",
    "        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, \"\", new_query, \"\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scrape_google_repos():\n",
    "    url = ORG_REPOS_URL\n",
    "    all_repos = []\n",
    "    seen_names = set()\n",
    "    visited_urls = set()\n",
    "    empty_pages_in_a_row = 0\n",
    "    page_index = 0\n",
    "\n",
    "    while url:\n",
    "        if url in visited_urls:\n",
    "            print(f\"[Stop] Already visited: {url}\")\n",
    "            break\n",
    "        visited_urls.add(url)\n",
    "\n",
    "        print(f\"[Fetch] page {page_index+1}: {url}\")\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        time.sleep(1)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"[Error] Failed to fetch {url}: {resp.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        repos = []\n",
    "        for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "            if \"itemprop='owns'\" in c or \"data-testid='results-list'\" in c:\n",
    "                inner = BeautifulSoup(c, \"html.parser\")\n",
    "                repos.extend(extract_repos_from_page(inner))\n",
    "\n",
    "        repos.extend(extract_repos_from_page(soup))\n",
    "\n",
    "        new_count = 0\n",
    "        for r in repos:\n",
    "            if r[\"name\"] not in seen_names:\n",
    "                all_repos.append(r)\n",
    "                seen_names.add(r[\"name\"])\n",
    "                new_count += 1\n",
    "\n",
    "        print(f\"[Parsed] new repos: {new_count}, total: {len(all_repos)}\")\n",
    "\n",
    "        if new_count == 0:\n",
    "            empty_pages_in_a_row += 1\n",
    "        else:\n",
    "            empty_pages_in_a_row = 0\n",
    "\n",
    "        if empty_pages_in_a_row >= 3:\n",
    "            print(\"[Stop] No new repos for 3 pages in a row. Ending.\")\n",
    "            break\n",
    "\n",
    "        page_index += 1\n",
    "\n",
    "        next_url = get_next_page_url(soup, url)\n",
    "        if not next_url:\n",
    "            print(\"[Stop] No next page.\")\n",
    "            break\n",
    "        if next_url == url:\n",
    "            print(f\"[Stop] Next page equals current URL ({next_url}).\")\n",
    "            break\n",
    "        if next_url in visited_urls:\n",
    "            print(f\"[Stop] Next page already visited ({next_url}).\")\n",
    "            break\n",
    "\n",
    "        print(f\"[Next] {next_url}\")\n",
    "        url = next_url\n",
    "\n",
    "    return all_repos\n",
    "\n",
    "def save_repos(conn, repos):\n",
    "    cur = conn.cursor()\n",
    "    for r in repos:\n",
    "        cur.execute(\n",
    "            \"INSERT OR IGNORE INTO repos (name, language, stars) VALUES (?, ?, ?)\",\n",
    "            (r[\"name\"], r[\"language\"], r[\"stars\"])\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def main():\n",
    "    conn = init_db()\n",
    "    repos = scrape_google_repos()\n",
    "    print(f\"Scraped {len(repos)} repositories.\")\n",
    "    save_repos(conn, repos)\n",
    "\n",
    "    print(\"Saved repos:\")\n",
    "    for row in conn.execute(\"SELECT name, language, stars FROM repos ORDER BY stars DESC, name ASC\"):\n",
    "        print(row)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
